# Qエージェント（Q学習）のリファクタリング計画

## 目的
- `QLearningAgent` の責務を分離し、読みやすさと変更容易性を上げる。
- 学習ロジックのテストしやすさを向上させる。
- 将来の拡張（報酬関数の差し替え、状態表現の変更、永続化）を低コスト化する。

## 現状の課題
1. **単一クラスへの責務集中**
   - 行動選択、エピソード実行、報酬計算、Q値更新、状態キー生成が `QLearningAgent` に集中している。
2. **外部依存が固定化されている**
   - `Math.random` に直接依存しており、再現可能なテストが書きにくい。
3. **Qテーブル表現が暗黙的**
   - `Map<string, number[]>` と `number[]` のインデックス規約に依存しており、意図がコード上で伝わりにくい。
4. **学習設定のバリデーション責務が分散しやすい**
   - 学習設定の整合性チェックをどこで担保するかが明確でない。

## リファクタリング方針

### 1) モジュール分割（責務分離）
以下の分割を行い、`QLearningAgent` をオーケストレーターに寄せる。

- `q-learning-policy.ts`
  - ε-greedy 戦略（探索/活用）を担当
- `q-learning-value-table.ts`
  - Qテーブルの取得・更新・初期化を担当
- `q-learning-reward.ts`
  - 勝敗から報酬値への変換を担当
- `q-learning-state-encoder.ts`
  - 盤面と手番から状態キーを生成
- `q-learning-trainer.ts`
  - 1エピソード実行と履歴更新（割引報酬の伝播）を担当

> 既存の `q-learning-agent.ts` は、`pickMove` と `train` の公開APIを維持しつつ、上記コンポーネントへ委譲する。

### 2) 依存性注入の導入
- 疑似乱数生成器（`RandomSource`）を注入可能にする。
- デフォルト実装は `Math.random` を利用し、既存挙動を保つ。
- テストではシード固定またはモック化した `RandomSource` を使用する。

### 3) 型安全性の向上
- Q値配列を `QValues` 型（例: `ReadonlyArray<number>` / `number[]` ラッパー）で明示化。
- `StateKey` 型エイリアスを導入し、文字列の意味を区別する。
- 学習設定は `ValidatedTrainingConfig` を用意し、境界値を吸収してから実行する。

### 4) テスト戦略の再編
- **ユニットテスト**
  - 方策（ε=0/1、同値行動のタイブレーク）
  - 報酬関数（勝ち/負け/引き分け）
  - 状態エンコード（手番違い・盤面違い）
  - Q値更新式（TD更新）
- **統合テスト**
  - 少数エピソードでQテーブルが更新されること
  - `pickMove` が利用可能セルのみ返すこと
- **回帰テスト**
  - 既存 `QLearningAgent` の公開インターフェース互換性を確認

## 実施ステップ

### Phase 0: 準備（半日）
- 既存テストの棚卸しと不足ケースの追加。
- 期待する外部API（`pickMove`, `train`, `clear`, `tableSize`）を固定。

### Phase 1: 安全な抽出（1日）
- `state-encoder` と `reward` を先に抽出。
- 既存テストをグリーン維持しながら機械的に置換。

### Phase 2: Qテーブル/方策の分離（1日）
- `value-table` と `policy` を抽出。
- タイブレーク・初期値挙動が変わらないことをテストで保証。

### Phase 3: トレーナー分離（1日）
- エピソード実行と履歴更新を `trainer` に移譲。
- ランダム依存を注入化して再現性テストを追加。

### Phase 4: 仕上げ（半日）
- 命名統一、重複コード削除、コメント整理。
- 将来拡張ポイント（永続化・ハイパーパラメータ探索）を TODO として明記。

## 完了条件（Definition of Done）
- `QLearningAgent` の公開APIが互換である。
- 既存テスト + 追加テストがすべて成功する。
- 分割した各モジュールに最低1件のユニットテストがある。
- 振る舞い変更がある場合、READMEまたはドキュメントに明記されている。

## リスクと対策
- **リスク**: 抽出時に学習挙動が微妙に変わる。
  - **対策**: フェーズごとに小さいコミットを積み、回帰テストを常時実行。
- **リスク**: 過度な抽象化で読みづらくなる。
  - **対策**: 「今使っている抽象」だけを導入し、未使用の汎化を避ける。
- **リスク**: テストが確率依存で不安定になる。
  - **対策**: 乱数注入を徹底し、ユニットテストは決定的にする。

## 期待効果
- バグ修正時の影響範囲が限定され、修正速度が上がる。
- 強化学習パラメータや報酬設計の実験がしやすくなる。
- 将来の AI エージェント追加時に再利用できる部品が増える。
