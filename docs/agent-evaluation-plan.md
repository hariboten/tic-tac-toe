# エージェント評価機能の計画

## 目的
- 既存の各エージェント（ランダム、モンテカルロ、ミニマックス、Q学習）を同一条件で比較できるようにする。
- 新規エージェント追加時に、性能退行を定量的に検知できる運用を作る。
- UI 上の「体感の強さ」だけでなく、再現可能な指標に基づいた意思決定を可能にする。

## スコープ
- 対象: 盤面サイズ 3x3 の三目並べ。
- 評価単位: 1対1の対戦セット（例: 1,000局）を1バッチとして集計。
- 除外: 学習アルゴリズムそのものの改修、マルチボード対応。

## マイルストーン
1. **M1: 最小評価基盤の実装（1日）**
   - CLI/スクリプトから対戦バッチを実行。
   - 勝率・引き分け率・平均手数を出力。
2. **M2: レポート整備（1日）**
   - JSON出力 + Markdownサマリー生成。
   - エージェント組み合わせごとの差分表示。
3. **M3: 回帰チェック導入（半日）**
   - 既存ベースラインとの差分閾値チェック。
   - CIで任意実行できる評価ジョブを追加。

## 実施手順
1. `AgentEvaluator` の骨格を追加。
2. シード固定の疑似乱数を導入し、評価の再現性を担保。
3. 対戦ランナーを作成し、全組み合わせ総当たりを実行。
4. 指標集計モジュールで結果を構造化。
5. レポート出力（JSON/Markdown）を実装。
6. ベースラインとの差分判定を追加。

## 完了条件（DoD）
- 同一シードで再実行した場合、評価結果が一致する。
- 主要指標（勝率・引き分け率・平均手数）が出力される。
- 2エージェント以上の比較結果を1コマンドで取得できる。
- 評価結果が docs または artifacts から参照可能な形式で保存される。

## リスクと対策
- 乱数依存で結果が揺れる: シード固定と複数バッチ平均で吸収。
- 実行時間が長い: デフォルト局数を段階化（quick/standard/full）する。
- 指標の解釈違い: 用語定義を仕様書に明記する。
